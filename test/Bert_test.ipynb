{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Bert_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Testing BERT Model\n",
        "\n",
        "Since we will use Bert to embed document contexts, we need to test bert model. \\\n",
        "As namuwiki is based on Korean, we will use KOBert(distilled ver, <https://github.com/monologg/DistilKoBERT>) \\\n",
        "This Code is tested on Colab, So it might be not available on other environments.\n",
        "\n"
      ],
      "metadata": {
        "id": "GtB4L9yiT5fo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "!pip install mxnet\r\n",
        "!pip install gluonnlp pandas tqdm\r\n",
        "!pip install sentencepiece\r\n",
        "!pip install transformers"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.23)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6IIf4VmTwQl",
        "outputId": "72387af0-1a21-4b11-baf1-bbb840334cea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "!git clone https://github.com/monologg/DistilKoBERT\r\n",
        "\r\n",
        "%cd DistilKoBERT"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DistilKoBERT' already exists and is not an empty directory.\n",
            "/content/DistilKoBERT\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meVXbJk-T5AY",
        "outputId": "e1f9cc90-9211-44ab-bedf-81c6441f0a20"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "!wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\r\n",
        "!wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-08-07 19:19:59--  https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/374ftkec978br3d/ratings_train.txt [following]\n",
            "--2021-08-07 19:19:59--  https://www.dropbox.com/s/dl/374ftkec978br3d/ratings_train.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce37f4ae2b18e17b8d9f08d98ca.dl.dropboxusercontent.com/cd/0/get/BTyVBc-3BiYHyQZVkUTjabD55k23x-xkYyFzK40nviaXf53-adv68ULMbNc5sClBEYxPAQzgR3BTCS3rOcjT3aXeJThNfOu01v72Lg05WPI5dWoG2aA_UlWDEseC1XvJ0GD5YoXI-jlEG4PXRWR9vu08/file?dl=1# [following]\n",
            "--2021-08-07 19:19:59--  https://uce37f4ae2b18e17b8d9f08d98ca.dl.dropboxusercontent.com/cd/0/get/BTyVBc-3BiYHyQZVkUTjabD55k23x-xkYyFzK40nviaXf53-adv68ULMbNc5sClBEYxPAQzgR3BTCS3rOcjT3aXeJThNfOu01v72Lg05WPI5dWoG2aA_UlWDEseC1XvJ0GD5YoXI-jlEG4PXRWR9vu08/file?dl=1\n",
            "Resolving uce37f4ae2b18e17b8d9f08d98ca.dl.dropboxusercontent.com (uce37f4ae2b18e17b8d9f08d98ca.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6032:15::a27d:520f\n",
            "Connecting to uce37f4ae2b18e17b8d9f08d98ca.dl.dropboxusercontent.com (uce37f4ae2b18e17b8d9f08d98ca.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14628807 (14M) [application/binary]\n",
            "Saving to: ‘ratings_train.txt?dl=1.4’\n",
            "\n",
            "ratings_train.txt?d 100%[===================>]  13.95M  59.6MB/s    in 0.2s    \n",
            "\n",
            "2021-08-07 19:20:00 (59.6 MB/s) - ‘ratings_train.txt?dl=1.4’ saved [14628807/14628807]\n",
            "\n",
            "--2021-08-07 19:20:00--  https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/977gbwh542gdy94/ratings_test.txt [following]\n",
            "--2021-08-07 19:20:00--  https://www.dropbox.com/s/dl/977gbwh542gdy94/ratings_test.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc0d385004b889e7d777b5494c71.dl.dropboxusercontent.com/cd/0/get/BTz-hdsFxemHpf8DPjidXYBdQyhWjWZQ0jJPMAoQ-TP57BjjTL8Qo2-paCwNQS7gJghuGmianGfUQTSehwqrvkbicupOjB9IX2Jm_1dUj90TJRv2FqwU8o1FNYs_S3tr2zNNeWCwhiFuPfhqM_WIgfP-/file?dl=1# [following]\n",
            "--2021-08-07 19:20:00--  https://uc0d385004b889e7d777b5494c71.dl.dropboxusercontent.com/cd/0/get/BTz-hdsFxemHpf8DPjidXYBdQyhWjWZQ0jJPMAoQ-TP57BjjTL8Qo2-paCwNQS7gJghuGmianGfUQTSehwqrvkbicupOjB9IX2Jm_1dUj90TJRv2FqwU8o1FNYs_S3tr2zNNeWCwhiFuPfhqM_WIgfP-/file?dl=1\n",
            "Resolving uc0d385004b889e7d777b5494c71.dl.dropboxusercontent.com (uc0d385004b889e7d777b5494c71.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6032:15::a27d:520f\n",
            "Connecting to uc0d385004b889e7d777b5494c71.dl.dropboxusercontent.com (uc0d385004b889e7d777b5494c71.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4893335 (4.7M) [application/binary]\n",
            "Saving to: ‘ratings_test.txt?dl=1.4’\n",
            "\n",
            "ratings_test.txt?dl 100%[===================>]   4.67M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-08-07 19:20:01 (31.4 MB/s) - ‘ratings_test.txt?dl=1.4’ saved [4893335/4893335]\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSY8t_dDXmeI",
        "outputId": "396f2caf-1356-4a5f-a389-ff19a09d64ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Libraries\n",
        "\n",
        "Using transformers, we can load model easily."
      ],
      "metadata": {
        "id": "xpwiQlJFWw2I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "import gluonnlp as nlp\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from tokenization_kobert import KoBertTokenizer\r\n",
        "from transformers import DistilBertModel"
      ],
      "outputs": [],
      "metadata": {
        "id": "XUo6iiXHWx53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pretrained KoBert Model"
      ],
      "metadata": {
        "id": "XAXYVkOAbZFS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\r\n",
        "bert = DistilBertModel.from_pretrained('monologg/distilkobert')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'KoBertTokenizer'.\n",
            "Some weights of the model checkpoint at monologg/distilkobert were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr8ZBjC-bZf6",
        "outputId": "45c11544-8c19-41a6-a55d-a387fe512638"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Use tokenizer, tokenize given string\n",
        "# By use tokenizer.encode we can encoding string to token-idx and can easily padding\n",
        "\n",
        "test = '한국어 모델을 공유합니다.'\n",
        "\n",
        "token = tokenizer.encode(test)\n",
        "\n",
        "token"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8j8bGZFba0I",
        "outputId": "bf1ef185-6a60-4aee-fe43-a0cb291aef25"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Read data\n",
        "\n",
        "dataset_train = nlp.data.TSVDataset(\"ratings_train.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)\n",
        "dataset_test = nlp.data.TSVDataset(\"ratings_test.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)\n",
        "\n",
        "dataset_test[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['굳 ㅋ', '1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuogg7wDkGRq",
        "outputId": "c7bdf938-b9e2-43b4-a200-07f8b1307cbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Dataset\n",
        "\n",
        "To train model, we have to process original data with tokenizer. \\\n",
        "For this propose, use Dataset class in pyotrch."
      ],
      "metadata": {
        "id": "YhNgse5KloYZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "class Rating(Dataset):\n",
        "  def __init__(self, data, tokenizer, max_len):\n",
        "    self.comments = [tokenizer(d[0], padding='max_length', max_length=max_len, return_tensors= 'pt', truncation=True) for d in data]\n",
        "    self.labels = [d[1] for d in data]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.comments[idx], torch.tensor([int(self.labels[idx])])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ],
      "outputs": [],
      "metadata": {
        "id": "p2z4xlFUlojq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# Trian settings\n",
        "epoch = 5\n",
        "max_len = 64\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "log_interval = 500\n",
        "max_grad_norm = 0.5\n",
        "\n",
        "device = 'cuda'"
      ],
      "outputs": [],
      "metadata": {
        "id": "m3kT39gUPJqm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "dataset_train = Rating(dataset_train, tokenizer, max_len)\n",
        "dataset_test = Rating(dataset_test, tokenizer, max_len)\n",
        "\n",
        "trainLoader = DataLoader(dataset_train, batch_size=batch_size, num_workers=5)\n",
        "testLoader = DataLoader(dataset_test, batch_size=batch_size, num_workers=5)\n",
        "\n",
        "dataset_test[0]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_ids': tensor([[   2,  517, 5515,  517,  492,    3,    1,    1,    1,    1,    1,    1,\n",
              "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "             1,    1,    1,    1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}, tensor([1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI6q2Op7YAZ9",
        "outputId": "6d79003d-2d7d-4325-b807-6131797b5510"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "# To test tokenizer works correct, reverse data\n",
        "\n",
        "tokenizer.convert_ids_to_tokens(dataset_test[0][0]['input_ids'][0])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " '▁',\n",
              " '굳',\n",
              " '▁',\n",
              " 'ᄏ',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2S6Q5kGb1sv",
        "outputId": "db73e429-b590-4299-aa3e-85fc7d14cb11"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# Test model\n",
        "\n",
        "output = bert(input_ids=dataset_test[0][0]['input_ids'], attention_mask=dataset_test[0][0]['attention_mask'])\n",
        "\n",
        "# Output of DistillBert is sequence_output, (hidden_states), (attentions)\n",
        "# To get embeddign of [CLS] token, sequence_output[0][:, 0]\n",
        "output[0][:, 0].size()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY-eRYVTsA0i",
        "outputId": "1b4db36b-3b1e-4d54-8c5b-da0eefef25e1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Mdoel\n",
        "\n",
        "Bert model itself is not predication model. It is more like embedding model. \\\n",
        "To predict review rating, we need to construct classification model based on Bert."
      ],
      "metadata": {
        "id": "UYVOF5MAP0l2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, bert, hidden_layers=[512], num_classes=2):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    --------------\n",
        "\n",
        "    bert : Bert model\n",
        "      Bert Model to imbed input text\n",
        "    hidden_layers : list(int)\n",
        "      Layers of classifier, each int is size of each layers\n",
        "    num_classes : int\n",
        "      output classes\n",
        "    \"\"\"\n",
        "    super(Classifier, self).__init__()\n",
        "    self.bert = bert\n",
        "\n",
        "    layers = []\n",
        "    # Output size of bert model\n",
        "    input_size = 768\n",
        "\n",
        "    layers += [nn.BatchNorm1d(input_size),\n",
        "               nn.ReLU(inplace=True)]\n",
        "\n",
        "    for layer_size in hidden_layers:\n",
        "      layers += [nn.Linear(input_size, layer_size),\n",
        "                          nn.BatchNorm1d(layer_size),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                          nn.Dropout(p=0.5)]\n",
        "\n",
        "      input_size = layer_size\n",
        "    \n",
        "    layers.append(nn.Linear(input_size, num_classes))\n",
        "\n",
        "    self.hidden = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    x = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0][:, 0]\n",
        "\n",
        "    x = self.hidden(x)\n",
        "\n",
        "    return x\n",
        "    "
      ],
      "outputs": [],
      "metadata": {
        "id": "yuvTR5TDQMbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "model = Classifier(bert).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "outputs": [],
      "metadata": {
        "id": "WLGpAVIhKugy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model\n"
      ],
      "metadata": {
        "id": "bzqeF1ynPKxm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "opt = optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "outputs": [],
      "metadata": {
        "id": "5igZFV31PvXm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "def Cal_accuracy(pred, y):\n",
        "  pred_label = torch.argmax(pred, dim=1)\n",
        "  acc = (pred_label == y).sum().data.cpu() / (len(y) + 1)\n",
        "  return acc"
      ],
      "outputs": [],
      "metadata": {
        "id": "KQ_XUPWoyuSk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "# To plot loss save losses\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for e in range(epoch):\n",
        "  model.train()\n",
        " \n",
        "  i = 0\n",
        "  for inputs, labels in trainLoader:\n",
        "    opt.zero_grad()\n",
        "\n",
        "    input_ids = inputs['input_ids'].squeeze().to(device)\n",
        "    attention_mask = inputs['attention_mask'].squeeze().to(device)\n",
        "\n",
        "    labels = labels.squeeze().to(device)\n",
        "\n",
        "    pred = model(input_ids, attention_mask)\n",
        "    loss = loss_fn(pred, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    opt.step()\n",
        "\n",
        "    train_loss.append(loss.mean().data.cpu())\n",
        "    acc = Cal_accuracy(pred, labels)\n",
        "\n",
        "    i += 1\n",
        "    if (i % log_interval) == 0:\n",
        "      print(f\"epoch {e} train_loss {loss.data.cpu()} acc {acc}\")\n",
        "  \n",
        "  model.eval()\n",
        "  i = 0\n",
        "  losses = 0\n",
        "  acc = 0\n",
        "  for inputs, labels in testLoader:\n",
        "    input_ids = inputs['input_ids'].squeeze().to(device)\n",
        "    attention_mask = inputs['attention_mask'].squeeze().to(device)\n",
        "\n",
        "    labels = labels.squeeze().to(device)\n",
        "\n",
        "    pred = model(input_ids, attention_mask)\n",
        "    loss = loss_fn(pred, labels)\n",
        "    losses += loss.sum().data.cpu()\n",
        "    acc += Cal_accuracy(pred, labels)\n",
        "\n",
        "    test_loss.append(loss.mean().data.cpu())\n",
        "    i += 1\n",
        "  \n",
        "  losses = losses / (batch_size * i)\n",
        "  acc = acc / i\n",
        "\n",
        "  print(f\"epoch {e} test_loss {losses} acc {acc}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 train_loss 0.3301074206829071 acc 0.8449612259864807\n",
            "epoch 0 train_loss 0.24193592369556427 acc 0.8759689927101135\n",
            "epoch 0 test_loss 0.0027106255292892456 acc 0.8405233025550842\n",
            "epoch 1 train_loss 0.29001665115356445 acc 0.8682170510292053\n",
            "epoch 1 train_loss 0.18155460059642792 acc 0.9224806427955627\n",
            "epoch 1 test_loss 0.002999036107212305 acc 0.834829568862915\n",
            "epoch 2 train_loss 0.24404986202716827 acc 0.8914728760719299\n",
            "epoch 2 train_loss 0.15430374443531036 acc 0.9457364082336426\n",
            "epoch 2 test_loss 0.003221197286620736 acc 0.8367566466331482\n",
            "epoch 3 train_loss 0.1998506486415863 acc 0.9379844665527344\n",
            "epoch 3 train_loss 0.1112002432346344 acc 0.9534883499145508\n",
            "epoch 3 test_loss 0.004358542151749134 acc 0.8123865127563477\n",
            "epoch 4 train_loss 0.14493706822395325 acc 0.930232584476471\n",
            "epoch 4 train_loss 0.06852833926677704 acc 0.9689922332763672\n",
            "epoch 4 test_loss 0.003728030016645789 acc 0.8272156119346619\n"
          ]
        }
      ],
      "metadata": {
        "id": "8qNCbckHPMEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0bf23be-5518-4ec5-ef54-530833e0f7df"
      }
    }
  ]
}